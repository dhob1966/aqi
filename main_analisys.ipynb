{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Note on metric and hyperparameters*\n",
    "##### Given that our data exihibits certain periodicity is reasonable to use \n",
    "##### a mean square error metric since it to penalizes more larger errors than smaller ones\n",
    "\n",
    "##### The hyperparameters are picked based on\n",
    "##### avoiding overfitting or underfitting.\n",
    "##### This is attained by having a slightly higher score on the training data set respect to the test data set.\n",
    "##### High train score and lower test score indicates overfitting.\n",
    "##### High test score and lower train score indicates underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Note on the choosen interval:*\n",
    "##### We test daily and montly predictions \n",
    "##### provided that daily data is significantly larger than monthly data\n",
    "##### we can have a more appropiated modelling for such a escale.\n",
    "##### If the data holds this kind of pattern for larger periods of time\n",
    "##### we should expect the monthly analysis to be similar to the daily one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Note on algorithms used*\n",
    "##### we find below that a Linear Regression model seems\n",
    "##### to work slightly better than Random Forests or XGBOOST algorithms\n",
    "##### this could be due to the rather lack of complexity in our data and \n",
    "##### the way we treated it.\n",
    "##### If we would be able to add more meaningful features to our model\n",
    "##### that would make more likely to work better for those algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Check functions.py documentation for more info*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going over all stations\n",
    "from functions import *\n",
    "stations =['AP001', 'AP005', 'AS001', 'BR005', 'BR006','BR007','BR008',\n",
    "       'BR009', 'BR010', 'CH001', 'DL001', 'DL002', 'DL003', 'DL004',\n",
    "       'DL005', 'DL006', 'DL007', 'DL008', 'DL009', 'DL010', 'DL011',\n",
    "       'DL012', 'DL013', 'DL014', 'DL015', 'DL016', 'DL017', 'DL018',\n",
    "       'DL019', 'DL020', 'DL021', 'DL022', 'DL023', 'DL024', 'DL025',\n",
    "       'DL026', 'DL027', 'DL028', 'DL029', 'DL030', 'DL031', 'DL032',\n",
    "       'DL033', 'DL034', 'DL035', 'DL036', 'DL037', 'DL038', 'GJ001',\n",
    "       'HR011', 'HR012', 'HR013', 'HR014', 'JH001', 'KA002', 'KA003',\n",
    "       'KA004', 'KA005', 'KA006', 'KA007', 'KA008', 'KA009', 'KA010',\n",
    "       'KA011', 'KL002', 'KL004', 'KL007', 'KL008', 'MH005', 'MH006',\n",
    "       'MH007', 'MH008', 'MH009', 'MH010', 'MH011', 'MH012', 'MH013',\n",
    "       'MH014', 'ML001', 'MP001', 'MZ001', 'OD001', 'OD002', 'PB001',\n",
    "       'RJ004', 'RJ005', 'RJ006', 'TG001', 'TG002', 'TG003', 'TG004',\n",
    "       'TG005', 'TG006', 'TN001', 'TN002', 'TN003', 'TN004', 'TN005',\n",
    "       'UP012', 'UP013', 'UP014', 'UP015', 'UP016', 'WB007', 'WB008',\n",
    "       'WB009', 'WB010', 'WB011', 'WB012', 'WB013']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Day Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to get only the rows at 8 am is used: \n",
    "#!awk '/08:00:00/{print $0}' station_hour.csv > station_hour_8am.csv \n",
    "# The time is choosen arbitarily just to have data each 24 hours.\n",
    "# (need to add the header: StationId,Datetime,PM2.5,PM10,NO,NO2,NOx,NH3,CO,SO2,O3,Benzene,Toluene,Xylene,AQI,AQI_Bucket)\n",
    "\n",
    "file=['station_hour_8am.csv']\n",
    "###HYPERPARAMETERS###\n",
    "num_boost = [5,10,15,25] #in case of XGBOOST. It stands for number of boosting rounds in case of usinf xgboost\n",
    "num_estim=[200,400,600] # in case of Random Forest. Number of estimators for random forest\n",
    "past_observation_prepro=[20,40,70] #number of past observations to compute outliers\n",
    "past_observations_model=[20,40,70] #number of past observations to use as predictors\n",
    "smooth=[2,3,5,10] #smoothing signal factor\n",
    "algor=['lr','random_for','xgboost'] # algorithm to be used\n",
    "path_model=['./models/model_day']\n",
    "\n",
    "\n",
    "#list with the r-squared metrics for train and test data\n",
    "rs_train=[]\n",
    "rs_test=[]\n",
    "train_data_list=[]\n",
    "test_pred_list=[]\n",
    "for i in range(len(stations)):\n",
    "    \n",
    "    try:    \n",
    "        rsquare=any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],\n",
    "        name=stations[i],N=smooth[0],ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "\n",
    "        rsquare_list=list(rsquare)\n",
    "        rs_train.append(rsquare_list[0][0])\n",
    "        rs_test.append(rsquare_list[0][1])\n",
    "        train_data_list.append(rsquare_list[0][2])\n",
    "        test_pred_list.append(rsquare_list[0][3])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "print(\"mean of squared roots of rsquare_train\")    \n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(\"median of squared roots of rsquare_train\")    \n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(\"mean of squared roots of rsquare_test\")    \n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(\"median of squared roots of rsquare_test\")    \n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "# ERROR meaning the station data did not have enough data to take the past_observations lags\n",
    "# or smoothing factor\n",
    "\n",
    "\n",
    "print('#########\\n############')\n",
    "\n",
    "# We use the best configuration Found  (check further below)\n",
    "\n",
    "'''\n",
    "returns array with score difference. If the score is 0 the prediction is accurate.\n",
    "If it's negative means that we predicted cleaner air quality than actually is. e.g. -1 could mean we predicted \"good\" air quality but in reality was just satisfactory.\n",
    "\n",
    "If it's positive means that we predicted less clean air quality than actually is. e.g. 1 could mean we predicted \"satisfactory\" air quality but in reality was good.\n",
    "\n",
    "The larger the number the larger we missed the right category (i.e. good,satifactpry, moderate, etc.)\n",
    "\n",
    "On these grounds we will prefer always a positive difference over a negative one.\n",
    "'''\n",
    "print(\"For first station on the list:\", stations[0])\n",
    "precision=list(precision_array(num_past_observations_model=past_observations_model[0],N=smooth[0],test_data_AQI=train_data_list[0], test_pred_AQI=test_pred_list[0],day=True))\n",
    "print(\"Air quality prediction\\n\",precision[0][0])\n",
    "print('#####################\\n#####################')\n",
    "print(\"precision array:\",precision[0][1])\n",
    "print('#####################\\n#####################')\n",
    "print('Train and test plots for all the stations:')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the best configuration Found (check further below)\n",
    "'''\n",
    "returns array with score difference. If the score is 0 the prediction is accurate.\n",
    "If it's negative means that we predicted cleaner air quality than actually is. e.g. -1 could mean we predicted \"good\" air quality but in reality was just satisfactory.\n",
    "\n",
    "If it's positive means that we predicted less clean air quality than actually is. e.g. 1 could mean we predicted \"satisfactory\" air quality but in reality was good.\n",
    "\n",
    "The larger the number the larger we missed the right category (i.e. good,satifactpry, moderate, etc.)\n",
    "\n",
    "On these grounds we will prefer always a positive difference over a negative one.\n",
    "'''\n",
    "print(\"For first station on the list:\", stations[0])\n",
    "precision=list(precision_array(num_past_observations_model=past_observations_model[0],N=smooth[0],test_data_AQI=train_data_list[0], test_pred_AQI=test_pred_list[0]))\n",
    "print(\"Air quality prediction\\n\",precision[0][0])\n",
    "print('#####################\\n#####################')\n",
    "print(\"precision array:\",precision[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Results for different Hyperparameters configurations**\n",
    "##### For this could be more convinient a gridsearch for hyperparameter tunning, however for timewise reasons\n",
    "##### and clarity we test manually some different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "###HYPERPARAMETERS###\n",
    "num_boost = [5,10,15,25] #in case of XGBOOST. It stands for number of boosting rounds in case of usinf xgboost\n",
    "num_estim=[200,400,600] # in case of Random Forest. Number of estimators for random forest\n",
    "past_observation_prepro=[20,40,70] #number of past observations to compute outliers\n",
    "past_observations_model=[20,40,70] #number of past observations to use as predictors\n",
    "smooth=[2,3,5,10] #smoothing signal factor\n",
    "algor=['lr','random_for','xgboost'] # algorithm to be used\n",
    "path_model=['./models/model_LR_v-1']\n",
    "\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],name=stations[i],N=smooth[2],ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "# To have a more intuitive result we print the square root of our metrics\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "9.945567740664915\n",
    "9.746615186625352\n",
    "43.00213386854126\n",
    "44.02297692287345\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[1],name=stations[i],N=smooth[2],ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "# To have a more intuitive result we print the square root of our metrics\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "9.800340465716236\n",
    "9.872106863987552\n",
    "42.16095054763079\n",
    "40.133623419359154\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[2],name=stations[i],N=smooth[2],ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "9.376791406032845\n",
    "9.525691364622407\n",
    "40.115734445776795\n",
    "40.19732814750828\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],name=stations[i],N=smooth[3],ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "5.497086780625923\n",
    "5.495244963003785\n",
    "44.66817712849665\n",
    "43.95780999458252\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],name=stations[i],N=smooth[0],ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "23.315223846433774\n",
    "22.684894016159095\n",
    "39.519808018544516\n",
    "39.294515129531895\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],name=stations[i],N=1,ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "45.524304074925695\n",
    "44.534034977083834\n",
    "40.17374749052567\n",
    "41.21088830132818\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],name=stations[i],N=smooth[0],ml_algor=algor[1],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[1])\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "11.536505675357168\n",
    "11.169206142780531\n",
    "43.023179078687384\n",
    "44.08070734781663\n",
    "\n",
    "\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],name=stations[i],N=smooth[0],ml_algor=algor[1],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[1],boost_round=num_boost[1])\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "11.475091958011134\n",
    "10.917182799036068\n",
    "43.006824114037954\n",
    "43.36383669884698\n",
    "\n",
    "#RESULTS FOR THE CONFIGURATION:\n",
    "any_station_day(general_file=file[0],num_past_observation_prepro=past_observation_prepro[0],num_past_observations_model=past_observations_model[0],name=stations[i],N=smooth[0],ml_algor=algor[2],path_store_model=path_model[0]+'_'+str(stations[i])+'.pkl',num_estim_rf=num_estim[1],boost_round=num_boost[1])\n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "14.531872378601587\n",
    "13.40621135839562\n",
    "43.245711080827476\n",
    "44.14326580666763\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prediction for the next Month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going over all stations\n",
    "from functions import *\n",
    "stations =['AP001', 'AP005', 'AS001', 'BR005', 'BR006','BR007','BR008',\n",
    "       'BR009', 'BR010', 'CH001', 'DL001', 'DL002', 'DL003', 'DL004',\n",
    "       'DL005', 'DL006', 'DL007', 'DL008', 'DL009', 'DL010', 'DL011',\n",
    "       'DL012', 'DL013', 'DL014', 'DL015', 'DL016', 'DL017', 'DL018',\n",
    "       'DL019', 'DL020', 'DL021', 'DL022', 'DL023', 'DL024', 'DL025',\n",
    "       'DL026', 'DL027', 'DL028', 'DL029', 'DL030', 'DL031', 'DL032',\n",
    "       'DL033', 'DL034', 'DL035', 'DL036', 'DL037', 'DL038', 'GJ001',\n",
    "       'HR011', 'HR012', 'HR013', 'HR014', 'JH001', 'KA002', 'KA003',\n",
    "       'KA004', 'KA005', 'KA006', 'KA007', 'KA008', 'KA009', 'KA010',\n",
    "       'KA011', 'KL002', 'KL004', 'KL007', 'KL008', 'MH005', 'MH006',\n",
    "       'MH007', 'MH008', 'MH009', 'MH010', 'MH011', 'MH012', 'MH013',\n",
    "       'MH014', 'ML001', 'MP001', 'MZ001', 'OD001', 'OD002', 'PB001',\n",
    "       'RJ004', 'RJ005', 'RJ006', 'TG001', 'TG002', 'TG003', 'TG004',\n",
    "       'TG005', 'TG006', 'TN001', 'TN002', 'TN003', 'TN004', 'TN005',\n",
    "       'UP012', 'UP013', 'UP014', 'UP015', 'UP016', 'WB007', 'WB008',\n",
    "       'WB009', 'WB010', 'WB011', 'WB012', 'WB013']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inside the function any_station_month() there is a preprocessing done \n",
    "##### in order to get the AQI median value for each 15th day of a month.\n",
    "##### The 15_th day is choosen arbitrarily just to have month-like data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use awk to get only the data corresponding to a 15th day\n",
    "#awk '/-15/{print $0}' station_hour.csv > station_hour_15th.csv\n",
    "\n",
    "\n",
    "\n",
    "file=['station_hour_15th.csv']\n",
    "###HYPERPARAMETERS###\n",
    "num_boost = [5,10,15,25] #in case of XGBOOST. It stands for number of boosting rounds in case of usinf xgboost\n",
    "num_estim=[200,400,600] # in case of Random Forest. Number of estimators for random forest\n",
    "past_observations_model=[3,5,10] #number of past observations to use as predictors\n",
    "algor=['lr','random_for','xgboost'] # algorithm to be used\n",
    "path_model=['./models/model_month']\n",
    "\n",
    "\n",
    "#list with the r-squared metrics for train and test data\n",
    "rs_train=[]\n",
    "rs_test=[]\n",
    "train_data_list=[]\n",
    "test_pred_list=[]\n",
    "\n",
    "for i in range(len(stations)):\n",
    "    \n",
    "    try:    \n",
    "        rsquare=any_station_month(general_file=file[0],num_past_observations_model=past_observations_model[0],\n",
    "        name=stations[i],ml_algor=algor[0],path_store_model=path_model[0]+'_'+str(stations[0])+'.pkl',num_estim_rf=num_estim[0],boost_round=num_boost[0])\n",
    "\n",
    "        rsquare_list=list(rsquare)\n",
    "        rs_train.append(rsquare_list[0][0])\n",
    "        rs_test.append(rsquare_list[0][1])\n",
    "        train_data_list.append(rsquare_list[0][2])\n",
    "        test_pred_list.append(rsquare_list[0][3])\n",
    "    except IndexError:\n",
    "        pass\n",
    "print(\"mean of squared roots of rsquare_train\")    \n",
    "print(statistics.mean(np.sqrt(rs_train)))\n",
    "print(\"median of squared roots of rsquare_train\")    \n",
    "print(statistics.median(np.sqrt(rs_train)))\n",
    "print(\"mean of squared roots of rsquare_test\")    \n",
    "print(statistics.mean(np.sqrt(rs_test)))\n",
    "print(\"median of squared roots of rsquare_test\")    \n",
    "print(statistics.median(np.sqrt(rs_test)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"For first station on the list:\", stations[0])\n",
    "precision=list(precision_array(num_past_observations_model=past_observations_model[0],N=1,test_data_AQI=train_data_list[0], test_pred_AQI=test_pred_list[0],day=False))\n",
    "print(\"Air quality prediction\\n\",precision[0][0])\n",
    "print('#####################\\n#####################')\n",
    "print(\"precision array for the first station:\",precision[0][1])\n",
    "print('#####################\\n#####################')\n",
    "print('Train and test plots for all the stations:')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "690b2d0a49918924c1615e685712b44420f19b5ad87b42433ca814a96f74e896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
